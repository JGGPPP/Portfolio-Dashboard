{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info text-center\">\n",
    "    <H1> PHASE I (cont.) - preparing data </H1>\n",
    "</div>\n",
    "\n",
    "\n",
    "## Getting all dataframes ready to explore\n",
    "\n",
    "- Data In & Data Out\n",
    "- Getting the price history for each ticker\n",
    "- Getting business information to enrich the analysis\n",
    "- Understanding the big picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "from time import strftime, sleep\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from pandas_datareader import data as pdr\n",
    "from pandas.tseries.offsets import BDay\n",
    "import yfinance as yf\n",
    "yf.pdr_override()\n",
    "import BizExtractor\n",
    "\n",
    "def clean_header(df):\n",
    "    df.columns = df.columns.str.strip().str.lower().str.replace('.', '', regex=False).str.replace('(', \\\n",
    "                '', regex=False).str.replace(')', '', regex=False).str.replace(' ', '_', regex=False).str.replace('_/_', '/', regex=False)\n",
    "    \n",
    "def get_now():\n",
    "    now = datetime.now().strftime('%Y-%m-%d_%Hh%Mm')\n",
    "    return now\n",
    "\n",
    "def datetime_maker(df, datecol):\n",
    "    df[datecol] = pd.to_datetime(df[datecol])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the last transactions_finaldf from Phase I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_file = sorted(glob('../outputs/transactions_all/*finaldf*.xlsx'))[-1] # path to file in the folder\n",
    "print(last_file[-(len(last_file))+(last_file.rfind('/')+1):])\n",
    "all_transactions = pd.read_excel(last_file, engine='openpyxl')\n",
    "all_transactions.date = pd.to_datetime(all_transactions.date, format='%d/%m/%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tickers = list(all_transactions['ticker'].unique())\n",
    "print('You have {} different stocks'.format(len(all_tickers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All transactions without the delisted stocks\n",
    "# final_filtered = all_transactions[~all_transactions.ticker.isin(blacklist)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting the price history for all tickers\n",
    "\n",
    "- You can define the start date for the history below\n",
    "- Datareader will get each stock individually\n",
    "- all_data will have all the prices for every ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ly = datetime.today().year-1\n",
    "today = datetime.today()\n",
    "start_sp = datetime(2020, 1, 1)\n",
    "end_sp = today\n",
    "start_stocks = datetime(2020, 1, 1)\n",
    "end_stocks = today\n",
    "start_ytd = datetime(ly, 12, 31) + BDay(1) # to get the first business day\n",
    "\n",
    "def get(tickers, startdate, enddate):\n",
    "    def data(ticker):\n",
    "        return (pdr.get_data_yahoo(ticker, start=startdate, end=enddate))\n",
    "    datas = map(data, tickers)\n",
    "    return(pd.concat(datas, keys=tickers, names=['ticker', 'date']))\n",
    "               \n",
    "all_data = get(all_tickers, start_stocks, end_stocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_header(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.loc['TSLA']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the price history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "blacklist = []\n",
    "for tick in all_tickers:\n",
    "    try:\n",
    "        all_data.loc[tick].to_csv('../outputs/price_hist/{}_price_hist.csv'.format(tick))\n",
    "    except KeyError:\n",
    "        blacklist.append(tick)\n",
    "        print(f'Ticker {tick} has no price history to save.')\n",
    "        pass\n",
    "blacklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt_tickers = [tick for tick in all_tickers if tick not in blacklist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEGA_DICT = {}  \n",
    "min_date = '2020-01-01'  # optional\n",
    "TX_COLUMNS = ['date','ticker', 'cashflow', 'cml_units', 'cml_cost', 'gain_loss']\n",
    "tx_filt = all_transactions[TX_COLUMNS]  # keeping just the most relevant ones for now\n",
    "\n",
    "for ticker in filt_tickers:\n",
    "    prices_df = all_data[all_data.index.get_level_values('ticker').isin([ticker])].reset_index()\n",
    "    ## Can add more columns like volume!\n",
    "    PX_COLS = ['date', 'adj_close']\n",
    "    prices_df = prices_df[prices_df.date >= min_date][PX_COLS].set_index(['date'])\n",
    "    # Making sure we get sameday transactions\n",
    "    tx_df = tx_filt[tx_filt.ticker==ticker].groupby('date').agg({'cashflow': 'sum',\n",
    "                                                                 'cml_units': 'last',\n",
    "                                                                 'cml_cost': 'last',\n",
    "                                                                 'gain_loss': 'sum'})\n",
    "    # Merging price history and transactions dataframe\n",
    "    tx_and_prices = pd.merge(prices_df, tx_df, how='outer', left_index=True, right_index=True).fillna('-')\n",
    "    # This is to fill the days that were not in our transaction dataframe\n",
    "    tx_and_prices['cml_units'] = tx_and_prices['cml_units'].replace(to_replace='-', method='ffill')\n",
    "    tx_and_prices['cml_cost'] = tx_and_prices['cml_cost'].replace(to_replace='-', method='ffill')\n",
    "    tx_and_prices['gain_loss'] = tx_and_prices['gain_loss'].replace(to_replace='-', method='ffill')\n",
    "    tx_and_prices['cml_units'] = tx_and_prices['cml_units'].replace(to_replace='-', value=0)\n",
    "    tx_and_prices['cml_cost'] = tx_and_prices['cml_cost'].replace(to_replace='-', value=0)\n",
    "    tx_and_prices['gain_loss'] = tx_and_prices['gain_loss'].replace(to_replace='-', value=0)\n",
    "    tx_and_prices['cashflow'] = tx_and_prices['cashflow'].replace(to_replace='-', value=0)\n",
    "    # Cumulative sum for the cashflow\n",
    "    tx_and_prices['cashflow'] = tx_and_prices['cashflow'].cumsum()\n",
    "    tx_and_prices[['cml_cost', 'cml_units']] = tx_and_prices[['cml_cost', 'cml_units']].apply(pd.to_numeric)\n",
    "    tx_and_prices['avg_price'] = round(tx_and_prices['cml_cost']/tx_and_prices['cml_units'],3)\n",
    "    tx_and_prices['mktvalue'] = round(tx_and_prices['cml_units']*tx_and_prices['adj_close'],3)\n",
    "    tx_and_prices = tx_and_prices.add_prefix(ticker+'_')\n",
    "    # Once we're happy with the dataframe, add it to the dictionary\n",
    "    MEGA_DICT[ticker] = tx_and_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEGA_DICT['AAPL'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEGA_DF = pd.concat(MEGA_DICT.values(), axis=1)\n",
    "MEGA_DF.to_csv('../outputs/mega/MEGA_DF_{}.csv'.format(get_now()))\n",
    "MEGA_DF.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_file = sorted(glob('../outputs/mega/MEGA*.csv'))[-1] # path to file in the folder\n",
    "print(last_file[-(len(last_file))+(last_file.rfind('/')+1):])\n",
    "MEGA_DF = pd.read_csv(last_file)\n",
    "\n",
    "MEGA_DF['date'] = pd.to_datetime(MEGA_DF['date'])\n",
    "MEGA_DF.set_index('date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the daily snapshots of our portfolio\n",
    "\n",
    "- Taking the MEGA_DF dataframe, we keep just the \"market value\" column for each stock\n",
    "- If we sum the rows, it will show us how much our portfolio was worth on that day\n",
    "- Adding SP500 for reference and calculating some metrics\n",
    "- saving the portf_allvalues dataframe as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "portf_allvalues = MEGA_DF.filter(regex='mktvalue').fillna(0)\n",
    "portf_allvalues['portf_value'] = portf_allvalues.sum(axis=1)\n",
    "portf_allvalues['portf_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the S&P500 price return\n",
    "# You can use other symbols. Look it up on yahoo finance\n",
    "sp500 = pdr.get_data_yahoo('^GSPC', start_stocks, end_sp)\n",
    "clean_header(sp500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#getting the pct change\n",
    "portf_allvalues = portf_allvalues.join(sp500['adj_close'], how='inner')\n",
    "portf_allvalues.rename(columns={'adj_close': 'sp500_mktvalue'}, inplace=True)\n",
    "portf_allvalues['ptf_value_pctch'] = (portf_allvalues['portf_value'].pct_change()*100).round(2)\n",
    "portf_allvalues['sp500_pctch'] = (portf_allvalues['sp500_mktvalue'].pct_change()*100).round(2)\n",
    "portf_allvalues['ptf_value_diff'] = (portf_allvalues['portf_value'].diff()).round(2)\n",
    "portf_allvalues['sp500_diff'] = (portf_allvalues['sp500_mktvalue'].diff()).round(2)\n",
    "portf_allvalues.index.name = 'date'\n",
    "portf_allvalues.reset_index(inplace=True)\n",
    "portf_allvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portf_allvalues.to_csv('../outputs/portfolio_df/portfolio_df_{}.csv'.format(get_now()), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Sector and Industry\n",
    "### The \"give me everything\" method\n",
    "\n",
    "- In order for us to get a broader view on our portfolio, we want to add the sector and industry to the dataframe\n",
    "- If you want a complete view of each company, go ahead and use the GET_BIZ_DATA_ALL function\n",
    "- It takes a bit longer if you have a lot of tickers on your list, but it gives you a lot of information to explore later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "screener_all = BizExtractor.get_biz_data_all(all_tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visit https://finviz.com/screener.ashx\n",
    "\n",
    "screener_all.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "screener_all.to_csv('../outputs/ticker_information/ticker_information.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cols_clean_all = ['Ticker', 'Company', 'Sector', 'Industry', 'P/E', 'Perf Week', 'Perf Month', 'Perf Quart',\n",
    "             'Perf Half', 'Perf Year', 'Perf YTD', 'Volatility W', 'Volatility M', 'Recom', 'ATR',\n",
    "             'SMA20', 'SMA50', 'SMA200', '52W High', '52W Low', 'RSI', 'Insider Own', 'Insider Trans',\n",
    "             'Inst Own', 'Inst Trans', 'Float Short', 'Short Ratio', 'Dividend', 'LTDebt/Eq', 'Debt/Eq']\n",
    "screener_all_clean = screener_all[cols_clean_all]\n",
    "clean_header(screener_all_clean)\n",
    "screener_all_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "finviz_merged_raw = BizExtractor.get_finviz_screener('OVERVIEW', all_tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_clean = ['Ticker', 'Company', 'Sector', 'Industry', 'P/E']\n",
    "finviz_merged_clean = finviz_merged_raw[cols_clean]\n",
    "clean_header(finviz_merged_clean)\n",
    "finviz_merged_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Last Positions\n",
    "\n",
    "## Getting the latest values\n",
    "\n",
    "- In order to get the latest position value, we need to get the latest prices from yahoo finance\n",
    "- Since we also want sector and industry to be able to segment our portfolio, we will get data from finviz too\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_positions = all_transactions.groupby(['ticker']).agg({'cml_units': 'last', 'cml_cost': 'last',\n",
    "                                                'gain_loss': 'sum', 'cashflow': 'sum'}).reset_index()\n",
    "last_positions.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "curr_prices = []\n",
    "for tick in last_positions['ticker']:\n",
    "    try:\n",
    "        price = yf.download(tick, interval='1mo', period='d')['Adj Close'][-1]\n",
    "    except:\n",
    "        print(f'No price info for {tick}')\n",
    "        price = 0\n",
    "    curr_prices.append(price)\n",
    "len(curr_prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "last_positions['price'] = curr_prices\n",
    "last_positions['current_value'] = (last_positions.price * last_positions.cml_units).round(2)\n",
    "last_positions['avg_price'] = (last_positions.cml_cost / last_positions.cml_units).round(2)\n",
    "last_positions = last_positions.sort_values(by='current_value', ascending=False)\n",
    "\n",
    "last_positions.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Last Positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# final_lastpositions = pd.merge(finviz_merged_clean, last_positions, left_on='ticker', right_on='ticker', how='outer')\n",
    "final_lastpositions = pd.merge(screener_all_clean, last_positions, left_on='ticker', right_on='ticker', how='outer')\n",
    "final_lastpositions['current_value'] = final_lastpositions.price*final_lastpositions.cml_units\n",
    "final_lastpositions['avg_price'] = final_lastpositions.cml_cost/final_lastpositions.cml_units\n",
    "final_lastpositions['portf_weight'] = final_lastpositions.current_value/final_lastpositions.current_value.sum()*100\n",
    "final_lastpositions['unrealizedval'] = final_lastpositions.current_value - final_lastpositions.cml_cost\n",
    "final_lastpositions['unrealizedpct'] = final_lastpositions.unrealizedval / final_lastpositions.cml_cost*100\n",
    "final_lastpositions.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# final cosmetics\n",
    "final_lastpositions = final_lastpositions.replace('-', '0')\n",
    "for c in final_lastpositions.iloc[:,4:30].columns:\n",
    "    final_lastpositions[c] = pd.to_numeric(final_lastpositions[c].str.replace('%', ''))\n",
    "final_lastpositions.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_lastpositions.round(2).to_csv('../outputs/final_current_positions/final_current_positions_{}.csv'.format(get_now()), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouped DF's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_sect = final_lastpositions.groupby(['sector']).agg(\n",
    "    {'ticker': 'count', 'current_value': 'sum', 'cml_cost': 'sum', 'gain_loss': 'sum'}\n",
    ").sort_values(by='current_value', ascending= False).reset_index().round(2)\n",
    "grouped_sect['weight'] = round(grouped_sect.current_value/grouped_sect.current_value.sum()*100, 2)\n",
    "grouped_sect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_sect.to_csv('../outputs/grouped/grouped_sect_{}.csv'.format(get_now()), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grouped_sect_ind = final_lastpositions.groupby(['sector', 'industry']).agg(\n",
    "    {'ticker': 'count', 'current_value': 'sum', 'cml_cost': 'sum', 'gain_loss': 'sum'}\n",
    ").sort_values(by='current_value', ascending= False).reset_index().round(2)\n",
    "grouped_sect_ind['weight'] = round(grouped_sect_ind.current_value/grouped_sect_ind.current_value.sum()*100, 2)\n",
    "grouped_sect_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_sect_ind.to_csv('../outputs/grouped/grouped_sect_ind_{}.csv'.format(get_now()), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  5 2022, 01:53:17) \n[Clang 12.0.0 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "c4e4e0db8504a060df795c96eaaf5f2e40d78467421fda8e3c9314686cc061bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
